# -*- coding: utf-8 -*-
"""582-hw3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cvvi8oteEZt9VZJUwX6XTPJ_EXdCkiG3

# Data preparing
"""

import numpy as np
import struct
import matplotlib.pyplot as plt

with open('train-images.idx3-ubyte','rb') as f:
    magic, size = struct.unpack(">II", f.read(8))
    nrows, ncols = struct.unpack(">II", f.read(8))
    data = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))
    Xtraindata = np.transpose(data.reshape((size, nrows*ncols)))

with open('train-labels.idx1-ubyte','rb') as f:
    magic, size = struct.unpack(">II", f.read(8))
    data = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))
    ytrainlabels = data.reshape((size,)) # (Optional)

with open('t10k-images.idx3-ubyte','rb') as f:
    magic, size = struct.unpack(">II", f.read(8))
    nrows, ncols = struct.unpack(">II", f.read(8))
    data = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))
    Xtestdata = np.transpose(data.reshape((size, nrows*ncols)))

with open('t10k-labels.idx1-ubyte','rb') as f:
    magic, size = struct.unpack(">II", f.read(8))
    data = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))
    ytestlabels = data.reshape((size,)) # (Optional)



traindata_imgs =  np.transpose(Xtraindata).reshape((60000,28,28))
print('x-train shape:', Xtraindata.shape)
print('y-train shape:', ytrainlabels.shape)
print('x-test shape:', Xtestdata.shape)
print('y-test shape:', ytestlabels.shape)

import matplotlib.pyplot as plt

num_classes = 10
X_train = Xtraindata.T

# Plot the graph
fig, axes = plt.subplots(1, num_classes, figsize=(20, 20))

for i in range(num_classes):
    sample = X_train[ytrainlabels == i][0].reshape(28, 28)  # first images for each class
    axes[i].imshow(sample, cmap='gray')
    axes[i].set_title(f"Label: {i}", fontsize=16)
    axes[i].axis('off')

plt.show()

for i in range(10):
  print(ytrainlabels[i])

"""# TASK-1: PCA analysis"""

from sklearn.decomposition import PCA
X_train = Xtraindata.T
#print("X_train shape:", X_train.shape)

pca = PCA(n_components=16)  #choose 16 main components
pca.fit(X_train)

principal_components = pca.components_
print("main components shape:", principal_components.shape)  #checking the shape

import matplotlib.pyplot as plt
fig, axes = plt.subplots(4, 4, figsize=(8, 8))

for i, ax in enumerate(axes.flat):
    ax.imshow(principal_components[i].reshape(28, 28), cmap='grey')  # 784 = 28x28: reshaping
    ax.axis('off')
    ax.set_title(f'PC {i+1}')  #titles loke PC1, PC2, ...

plt.show()

"""# TASK-2: Determine k and plotting



"""

import numpy as np
import matplotlib.pyplot as plt

pca = PCA()  # analyzing all datas
pca.fit(X_train)

# calculating the variance equal to 85%
explained_variance = np.cumsum(pca.explained_variance_ratio_)

# fing the k when energy=85%
k = np.argmax(explained_variance >= 0.85) + 1

# plot the graph
plt.plot(explained_variance, marker='o')
plt.axhline(y=0.85, color='r', linestyle='--', label='85% threshold')  # line 85%
plt.axvline(x=k, color='g', linestyle='--', label=f'k = {k}')  # line k
plt.xlabel("k-components")
plt.ylabel("energy")
plt.title("finding k for 85% of energy")
plt.legend()
plt.show()

print(f"85% energy when k = {k}")

pca_k = PCA(n_components=k)  # using k = 59
X_train_pca = pca_k.fit_transform(X_train)
X_train_reconstructed = pca_k.inverse_transform(X_train_pca)

#choosing just 10 samples to comparing
n_samples = 10
fig, axes = plt.subplots(2, n_samples, figsize=(15, 4))

for i in range(n_samples):
    # original image
    axes[0, i].imshow(X_train[i].reshape(28, 28), cmap='gray')
    axes[0, i].axis('off')
    axes[0, i].set_title("original")

    # restored image
    axes[1, i].imshow(X_train_reconstructed[i].reshape(28, 28), cmap='gray')
    axes[1, i].axis('off')
    axes[1, i].set_title(f"k={k}")

plt.show()

"""#TASK-3,4: Digits 1 and 8 Using PCA and Ridge Classification"""

def select_digits(Xtraindata, ytrainlabels, Xtestdata, ytestlabels, digits):
    # transport data to get the appropriate shape
    X_train = Xtraindata.T
    X_test = Xtestdata.T

    #select the indexes of the necessary numbers in the traindata
    train_mask = np.isin(ytrainlabels, digits)
    X_subtrain = X_train[train_mask]
    y_subtrain = ytrainlabels[train_mask]
    #print(y_subtrain)

    # same method for the testdata
    test_mask = np.isin(ytestlabels, digits)
    X_subtest = X_test[test_mask]
    y_subtest = ytestlabels[test_mask]
    #print(y_subtest)

    return X_subtrain, y_subtrain, X_subtest, y_subtest

# let's choose 1 and 8
X_subtrain, y_subtrain, X_subtest, y_subtest = select_digits(Xtraindata, ytrainlabels, Xtestdata, ytestlabels, [1, 8])

# checking the shape of subsets
print("Shape of X_subtrain:", X_subtrain.shape)
print("Shape of y_subtrain:", y_subtrain.shape)
print("Shape of X_subtest:", X_subtest.shape)
print("Shape of y_subtest:", y_subtest.shape)

def plot_subset(X, y, num_images=16):

    fig, axes = plt.subplots(4, 4, figsize=(8, 8))

    for i, ax in enumerate(axes.flat):
        if i >= num_images:
            break
        ax.imshow(X[i].reshape(28, 28), cmap='gray')  # reshaping: 784=28x28
        ax.set_title(f"Label: {y[i]}", fontsize=10)
        ax.axis("off")  #

    plt.show()

# showing 16 images from subdatas
plot_subset(X_subtrain, y_subtrain)

from sklearn.linear_model import RidgeClassifier
from sklearn.model_selection import cross_val_score

# let's use k=59 from task 2
pca_k = PCA(n_components=59)
X_train_pca = pca_k.fit_transform(X_subtrain)
X_test_pca = pca_k.transform(X_subtest)
ridge_clf = RidgeClassifier(alpha=1.0)

# 5-fold cross-validation
cv_scores = cross_val_score(ridge_clf, X_train_pca, y_subtrain, cv=5)

print(f"Average cross-validation accuracy: {cv_scores.mean():.4f}")

from sklearn.metrics import accuracy_score

ridge_clf.fit(X_train_pca, y_subtrain)

# predict in test datas
y_pred = ridge_clf.predict(X_test_pca)

# evaluating accuracy
test_accuracy = accuracy_score(y_subtest, y_pred)
print(f"Test accuracy: {test_accuracy:.4f}")

# let's pick 16 images from our subset
num_images = 16
random_indices = np.random.choice(len(X_subtest), num_images, replace=False)

X_sample = X_subtest[random_indices]
y_true = y_subtest[random_indices]  # true
y_pred = ridge_clf.predict(X_test_pca[random_indices])  # predicted


fig, axes = plt.subplots(4, 4, figsize=(7, 7))

for i, ax in enumerate(axes.flat):
    ax.imshow(X_sample[i].reshape(28, 28), cmap='gray')  # reshape to 28√ó28
    ax.axis('off')
    ax.set_title(f"Real: {y_true[i]}\nPredicted: {y_pred[i]}", fontsize=10)

plt.tight_layout()
plt.show()

"""# TASK-5: digits 3,8 and 2,7"""

#choosing numbers 3, 8
X_subtrain_38, y_subtrain_38, X_subtest_38, y_subtest_38 = select_digits(Xtraindata, ytrainlabels, Xtestdata, ytestlabels, [3, 8])

#choosing numbers 2, 7
X_subtrain_27, y_subtrain_27, X_subtest_27, y_subtest_27 = select_digits(Xtraindata, ytrainlabels, Xtestdata, ytestlabels, [2, 7])

pca_k = PCA(n_components=59)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω–æ–µ —Ä–∞–Ω–µ–µ k=59

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º 3 vs. 8 –≤ PCA-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ
X_train_pca_38 = pca_k.fit_transform(X_subtrain_38)
X_test_pca_38 = pca_k.transform(X_subtest_38)

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º 2 vs. 7 –≤ PCA-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ
X_train_pca_27 = pca_k.fit_transform(X_subtrain_27)
X_test_pca_27 = pca_k.transform(X_subtest_27)

# Function for training the model, cross-validation, and testing
def train_and_evaluate(X_train_pca, y_train, X_test_pca, y_test):
    ridge_clf = RidgeClassifier(alpha=1.0)  # Initialize Ridge Classifier with alpha=1.0

    # Perform cross-validation (5-fold)
    cv_scores = cross_val_score(ridge_clf, X_train_pca, y_train, cv=5)
    print(f"Average cross-validation accuracy: {cv_scores.mean():.4f}")

    # Train on the entire training set and test on unseen data
    ridge_clf.fit(X_train_pca, y_train)
    y_pred = ridge_clf.predict(X_test_pca)

    # Calculate test accuracy
    test_accuracy = accuracy_score(y_test, y_pred)
    print(f"Test accuracy: {test_accuracy:.4f}")

    return cv_scores.mean(), test_accuracy  # Return cross-validation and test accuracy

# Classification for digit pair (3 vs. 8)
print("\nClassification 3 vs. 8:")
cv_38, test_38 = train_and_evaluate(X_train_pca_38, y_subtrain_38, X_test_pca_38, y_subtest_38)

# Classification for digit pair (2 vs. 7)
print("\nClassification 2 vs. 7:")
cv_27, test_27 = train_and_evaluate(X_train_pca_27, y_subtrain_27, X_test_pca_27, y_subtest_27)

# Function to train and evaluate different classifiers
def train_and_evaluate_model(model, model_name, X_train, y_train, X_test, y_test):
    # Perform cross-validation (5-fold)
    cv_scores = cross_val_score(model, X_train, y_train, cv=5)
    print(f"{model_name} - Cross-validation accuracy: {cv_scores.mean():.4f}")

    # Train on the entire training set and test on unseen data
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Calculate test accuracy
    test_accuracy = accuracy_score(y_test, y_pred)
    print(f"{model_name} - Test accuracy: {test_accuracy:.4f}\n")

    return cv_scores.mean(), test_accuracy

# Define models
ridge_clf = RidgeClassifier(alpha=1.0)  # Ridge Regression
knn_clf = KNeighborsClassifier(n_neighbors=5)  # KNN with k=5
lda_clf = LinearDiscriminantAnalysis()  # LDA

# Train and evaluate each model
print("\nüîπ Ridge Classifier Results:")
cv_ridge, test_ridge = train_and_evaluate_model(ridge_clf, "Ridge", X_train_pca, y_train, X_test_pca, y_test)

print("\nüîπ K-Nearest Neighbors (KNN) Results:")
cv_knn, test_knn = train_and_evaluate_model(knn_clf, "KNN", X_train_pca, y_train, X_test_pca, y_test)

print("\nüîπ Linear Discriminant Analysis (LDA) Results:")
cv_lda, test_lda = train_and_evaluate_model(lda_clf, "LDA", X_train_pca, y_train, X_test_pca, y_test)

"""# TASK-6+bonus"""

from sklearn.decomposition import PCA
from sklearn.linear_model import RidgeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score

X_train = Xtraindata.T
X_test = Xtestdata.T

# Print shapes to verify
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)

X_test_pca = pca.transform(X_test)
print("PCA-transformed X_train shape:", X_test_pca.shape)

# Function to train and evaluate different classifiers
def train_and_evaluate_model(model, model_name, X_train, y_train, X_test, y_test):
    # Perform cross-validation (5-fold)
    cv_scores = cross_val_score(model, X_train, y_train, cv=5)
    print(f"{model_name} - Cross-validation accuracy: {cv_scores.mean():.4f}")

    # Train on the entire training set and test on unseen data
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Calculate test accuracy
    test_accuracy = accuracy_score(y_test, y_pred)
    print(f"{model_name} - Test accuracy: {test_accuracy:.4f}\n")

    return cv_scores.mean(), test_accuracy

# Define models
ridge_clf = RidgeClassifier(alpha=1.0)  # Ridge Regression
knn_clf = KNeighborsClassifier(n_neighbors=5)  # KNN with k=5
lda_clf = LinearDiscriminantAnalysis()  # LDA

# Train and evaluate each model
print("\n Ridge Classifier Results:")
cv_ridge, test_ridge = train_and_evaluate_model(ridge_clf, "Ridge", X_train_pca, y_train, X_test_pca, y_test)

print("\n K-Nearest Neighbors (KNN) Results:")
cv_knn, test_knn = train_and_evaluate_model(knn_clf, "KNN", X_train_pca, y_train, X_test_pca, y_test)

print("\n Linear Discriminant Analysis (LDA) Results:")
cv_lda, test_lda = train_and_evaluate_model(lda_clf, "LDA", X_train_pca, y_train, X_test_pca, y_test)

from sklearn.svm import SVC

# Define SVM model with an RBF kernel (common for image classification)
svm_clf = SVC(kernel='rbf', C=10, gamma='scale')

# Train and evaluate SVM
print("\n Support Vector Machine (SVM) Results:")
cv_svm, test_svm = train_and_evaluate_model(svm_clf, "SVM", X_train_pca, y_train, X_test_pca, y_test)